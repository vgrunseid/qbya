Prospectos RAG Pipeline

Pipeline de Recuperaci√≥n Aumentada con Generaci√≥n (RAG) para prospectos m√©dicos. Convierte PDFs a texto, detecta secciones, genera chunks con metadata, indexa embeddings en Chroma, y responde preguntas usando un LLM local (Ollama o llama.cpp) con contexto citado.

üì¶ Estructura del proyecto
.
‚îú‚îÄ‚îÄ 1-pdf_to_txt.py        # PDFs ‚Üí TXT (Docling, opcional OCR Tesseract)
‚îú‚îÄ‚îÄ 2-transform.py         # TXT ‚Üí chunks.jsonl (detecci√≥n de secciones + metadata)
‚îú‚îÄ‚îÄ 3-embeddings.py        # chunks.jsonl ‚Üí Chroma (embeddings OpenAI o E5 local)
‚îú‚îÄ‚îÄ 4-retrieve.py          # Recuperaci√≥n (y respuesta con LLM local)
‚îú‚îÄ‚îÄ out_txt/               # TXT generados
‚îú‚îÄ‚îÄ out_chunks/            # chunks.jsonl
‚îî‚îÄ‚îÄ chroma/
    ‚îî‚îÄ‚îÄ prospectos/        # base persistida de Chroma

üõ†Ô∏è Requisitos

Python 3.10+

(Opcional) Tesseract para OCR:

macOS: brew install tesseract

Debian/Ubuntu: sudo apt-get install tesseract-ocr

(Opcional) Ollama para LLM local:

macOS: brew install ollama y luego ollama pull llama3.1

Dependencias Python:
pip install -r requirements.txt

Si us√°s OpenAI embeddings, defin√≠ OPENAI_API_KEY.

üöÄ Quickstart (de punta a punta)
# 0) instalar dependencias
pip install -r requirements.txt

# 1) PDFs ‚Üí TXT (usar --ocr es si hay escaneos)
python 1-pdf_to_txt.py -i pdfs_crudos -o out_txt --ocr es

# 2) TXT ‚Üí chunks.jsonl (con secciones + prefijo [SECCI√ìN: ...])
python 2-transform.py -i out_txt -o out_chunks/chunks.jsonl

# 3) Indexar en Chroma con E5 (local, sin API)
rm -rf chroma/prospectos
python 3-embeddings.py -f out_chunks/chunks.jsonl -p chroma/prospectos -c prospectos \
  --provider e5 --e5-model intfloat/multilingual-e5-base

# 4) Recuperar y responder con LLM local (Ollama)
ollama pull llama3.1
python 4-retrieve.py -q "contraindicaciones de ozempic" \
  -p chroma/prospectos -c prospectos \
  --provider e5 --e5-model intfloat/multilingual-e5-base \
  --auto --rerank keyword --answer --llm-backend ollama --llm-model llama3.1 -k 5

üó∫Ô∏è Diagrama del flujo
flowchart LR
  A[PDFs] --> B[1-pdf_to_txt.py\nDocling (+OCR opcional)]
  B --> C[TXT limpios\nout_txt/]
  C --> D[2-transform.py\nSecciones + chunks + metadata]
  D --> E[chunks.jsonl\nout_chunks/]
  E --> F[3-embeddings.py\nEmbeddings (OpenAI o E5)]
  F --> G[Chroma persistido\nchroma/prospectos/]
  G --> H[4-retrieve.py\nsimilaridad + filtros + (rerank)]
  H --> I[LLM local\n(Ollama/llama.cpp)\nrespuesta con citas]

üìò Detalle por script
1) 1-pdf_to_txt.py ‚Äî Convertir PDFs ‚Üí TXT

Convierte PDFs a UTF-8 con Docling. Si pas√°s --ocr, usa Tesseract (ideal para escaneos). Limpia NBSP/BOM y normaliza saltos.

Uso:

# sin OCR
python 1-pdf_to_txt.py

# con OCR en espa√±ol
python 1-pdf_to_txt.py --ocr es

# rutas personalizadas + OCR full page
python 1-pdf_to_txt.py -i ./pdfs_crudos -o ./out_txt --ocr es --force-ocr


Par√°metros clave:

-i/--input (default: ./pdfs_crudos)

-o/--output (default: ./out_txt)

--ocr es|eng|...

--force-ocr

--skip-existing

Salida: un .txt por PDF en out_txt/.

2) 2-transform.py ‚Äî TXT ‚Üí chunks.jsonl (secciones + metadata)

Detecta secciones (p. ej., POSOLOG√çA, CONTRAINDICACIONES), les asigna un nombre can√≥nico y chunkifica el contenido.
Si no encuentra encabezado confiable, aplica fallback por keywords en el cuerpo (p. ej., ‚Äúposolog√≠a/dosis/modo de administraci√≥n‚Äù, etc.).
Inserta un prefijo "[SECCI√ìN: ...]" al texto del chunk para mejorar la recuperaci√≥n.

Uso:

# defaults: in=./out_txt, out=./out_chunks/chunks.jsonl
python 2-transform.py

# tama√±os de chunk y solape
python 2-transform.py -i out_txt -o out_chunks/chunks.jsonl --chunk-size 1200 --chunk-overlap 200

# desactivar prefijo y fallback por cuerpo
python 2-transform.py --no-prefix-section --no-body-fallback

# fallback m√°s sensible (1 keyword basta)
python 2-transform.py --fallback-min-hits 1


Metadata por chunk:

drug_name (del nombre del archivo .txt, p. ej. ibupirac flex)

drug_root (primera palabra, p. ej. ibupirac)

doc_name (ej. ozempic.txt)

section_raw, section_canonical (o UNKNOWN)

section_match_score, section_inferred_from_body, section_infer_score

section_start_line, chunk_index_in_section

Salida: out_chunks/chunks.jsonl (una l√≠nea JSON por chunk).

3) 3-embeddings.py ‚Äî Indexar en Chroma

Lee el chunks.jsonl, calcula embeddings con OpenAI o E5 local y guarda todo en una colecci√≥n Chroma persistida.

Uso (E5 local):

python 3-embeddings.py -f out_chunks/chunks.jsonl -p chroma/prospectos -c prospectos \
  --provider e5 --e5-model intfloat/multilingual-e5-base


Uso (OpenAI):

export OPENAI_API_KEY=...
python 3-embeddings.py -f out_chunks/chunks.jsonl -p chroma/prospectos -c prospectos \
  --provider openai --openai-model text-embedding-3-small


Opciones √∫tiles:

--batch-size 256

--skip-existing (no reindexa IDs ya presentes)

-p y -c para manejar varias bases o colecciones

Salida: base en chroma/prospectos/ con vectores + metadata.

Importante: consult√° usando el mismo provider/modelo con el que indexaste (E5 ‚Üî E5, OpenAI ‚Üî OpenAI).

4) 4-retrieve.py ‚Äî Recuperaci√≥n (+ respuesta con LLM)

Recupera top-K fragmentos por similaridad (con filtros de metadata compatibles con tu Chroma), hace reranking para priorizar la secci√≥n pedida y, opcionalmente, usa un LLM local para sintetizar respuesta con citas [n].

Uso (solo recuperar):

python 4-retrieve.py -q "contraindicaciones de ozempic" \
  -p chroma/prospectos -c prospectos \
  --provider e5 --e5-model intfloat/multilingual-e5-base -k 5


Uso (recuperar + responder con Ollama):

ollama pull llama3.1
python 4-retrieve.py -q "posolog√≠a de sertal" \
  -p chroma/prospectos -c prospectos \
  --provider e5 --e5-model intfloat/multilingual-e5-base \
  --auto --rerank keyword --answer --llm-backend ollama --llm-model llama3.1 -k 5


Par√°metros clave:

Recuperaci√≥n: -q, -p, -c, --provider, --e5-model/--openai-model, -k

Filtros exactos: --drug (min√∫sculas), --section (MAY√öSCULAS)

Autodetecci√≥n desde la pregunta: --auto

Rerank: --rerank off|keyword|cross (para cross instalar sentence-transformers)

LLM local: --answer, --llm-backend ollama|llamacpp, --llm-model o --llm-model-path (.gguf), --llm-n-ctx, --llm-temperature, etc.

Salida: ranking de fragmentos y, si --answer, respuesta con citas + listado de fuentes.

üîç Consejos de calidad

Secciones en embeddings: el prefijo "[SECCI√ìN: ...]" dentro del chunk mejora notablemente la relevancia.

Filtros de Chroma: tu versi√≥n soporta "$eq", "$in", "$ne", "$gt", "$gte", "$lt", "$lte".
No uses "$contains". Para ‚Äúcontiene‚Äù, tra√© un lote y filtr√° en Python.

Secciones UNKNOWN: si faltan can√≥nicas (ej. POSOLOG√çA en Sertal), ajust√° SECTION_HINTS o baja --fallback-min-hits a 1 y reindex√°.

Nombres de drogas: drug_name proviene del nombre del archivo (ibupirac flex, sertal gotas). Para agrupar por ‚Äúfamilia‚Äù, us√° drug_root.

üß™ Verificaci√≥n opcional (debug)

Si quer√©s listar qu√© drogas y secciones hay en la colecci√≥n (√∫til para elegir filtros):

# drogas disponibles
from langchain_chroma import Chroma
emb = type("E", (), {"embed_documents": lambda *a, **k: [], "embed_query": lambda *a, **k: []})()
vs = Chroma(collection_name="prospectos", persist_directory="chroma/prospectos", embedding_function=emb)
res = vs._collection.get(include=["metadatas"], limit=100000).get("metadatas", [])
print(sorted({(m or {}).get("drug_name","") for m in res if m}))

# secciones para 'ozempic'
from collections import Counter
res = vs._collection.get(where={"drug_name":{"$eq":"ozempic"}}, include=["metadatas"], limit=100000)
cnt = Counter((m or {}).get("section_canonical") or (m or {}).get("section_raw") or "UNKNOWN" for m in res.get("metadatas",[]))
print(cnt.most_common())

üßØ Troubleshooting

No devuelve resultados:

Confirm√° que chroma/prospectos existe y count > 0.

Us√° el mismo embedding model que indexaste.

Prob√° sin filtros; luego agregalos de a uno.

Verific√° c√≥mo se llama exactamente drug_name (sale del filename).

Filtros rompen:

Us√° solo "$eq", "$in", "$and", "$or".

Si quer√©s ‚Äúcontiene‚Äù, filtr√° en Python tras traer un lote grande.

Secciones mal detectadas:

Ajust√° SECTION_HINTS en 2-transform.py o baj√° --fallback-min-hits.

Re-ejecut√° 2-transform.py y reindex√° (rm -rf chroma/prospectos; python 3-embeddings.py ...).

Ollama no responde:

ollama serve corriendo y modelo descargado (ollama pull llama3.1).

Si us√°s llama.cpp, asegurate de pasar --llm-model-path al .gguf.# qbya
